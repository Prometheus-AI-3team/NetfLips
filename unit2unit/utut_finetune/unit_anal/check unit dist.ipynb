{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8f8b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 150660 files in /home/2022113135/datasets/aihub_a2a_unit/train/ko...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5cf3b9e0df34b27926fb35b4531e3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 150660 files in /home/2022113135/datasets/aihub_a2a_unit/train/en...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c37638047104645a9651312a6059475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9416 files in /home/2022113135/datasets/aihub_a2a_unit/valid/ko...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ee41ea9ce49ad90782f1cc06c43e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9416 files in /home/2022113135/datasets/aihub_a2a_unit/valid/en...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18dad5f4e6d4115a1f95c210664642b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Counts saved to: /home/2022113135/jjs/notebooks/unit_counts_all_splits.json\n",
      "\n",
      "--- Summary ---\n",
      "train_ko   | Tokens: 34,545,370 | Unique Units: 969\n",
      "train_en   | Tokens: 39,805,808 | Unique Units: 909\n",
      "valid_ko   | Tokens: 2,134,670 | Unique Units: 931\n",
      "valid_en   | Tokens: 2,471,902 | Unique Units: 854\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# 1. Define paths\n",
    "base_dir = \"/home/2022113135/datasets/aihub_a2a_unit\"\n",
    "paths = {\n",
    "    \"train_ko\": os.path.join(base_dir, \"train/ko\"),\n",
    "    \"train_en\": os.path.join(base_dir, \"train/en\"),\n",
    "    \"valid_ko\": os.path.join(base_dir, \"valid/ko\"),\n",
    "    \"valid_en\": os.path.join(base_dir, \"valid/en\"),\n",
    "}\n",
    "\n",
    "output_file = \"unit_counts_all_splits.json\"\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Worker function to count units in a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            units = f.read().split()\n",
    "            return Counter(units)\n",
    "    except Exception as e:\n",
    "        return Counter()\n",
    "\n",
    "def get_counts_for_split(directory):\n",
    "    \"\"\"Efficiently collects unit frequencies from a directory.\"\"\"\n",
    "    file_paths = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    total_counter = Counter()\n",
    "    \n",
    "    print(f\"Processing {len(file_paths)} files in {directory}...\")\n",
    "    \n",
    "    # Process in chunks using all available CPU cores\n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        chunk_size = 2000\n",
    "        for i in tqdm(range(0, len(file_paths), chunk_size)):\n",
    "            chunk = file_paths[i:i + chunk_size]\n",
    "            results = executor.map(process_file, chunk)\n",
    "            for res in results:\n",
    "                total_counter.update(res)\n",
    "                \n",
    "    return dict(total_counter)\n",
    "\n",
    "# 2. Execute collection for all 4 splits\n",
    "all_results = {}\n",
    "for split_name, split_path in paths.items():\n",
    "    all_results[split_name] = get_counts_for_split(split_path)\n",
    "\n",
    "# 3. Save to JSON\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSuccess! Counts saved to: {os.path.abspath(output_file)}\")\n",
    "\n",
    "# Quick verification display\n",
    "print(\"\\n--- Summary ---\")\n",
    "for split, counts in all_results.items():\n",
    "    total_tokens = sum(counts.values())\n",
    "    unique_units = len(counts)\n",
    "    print(f\"{split:10} | Tokens: {total_tokens:,} | Unique Units: {unique_units}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25403a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

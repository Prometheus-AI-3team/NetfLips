arch: mbart_large
criterion: label_smoothed_cross_entropy
data: ./dataset
# ddp_backend: legacy_ddp
distributed_init_method: null
# distributed_no_spawn: false
distributed_num_procs: 7 #8
distributed_port: -1
distributed_world_size: 7 #8
label_smoothing: 0.2 # #################
langs: en,es,fr,it,pt,el,ru,cs,da,de,fi,hr,hu,lt,nl,pl,ro,sk,sl
# layernorm_embedding: true
log_interval: 100
report_accuracy: true
# lr:
# - 0.25
# lr_scheduler: polynomial_decay
max_update: 500000
# multilang_sampling_alpha: 1.0
nprocs_per_node: 7 #8
num_workers: 3
replace_length: -1 # ??????????????????????????? 원래는 1 (여러 token을 하나의 [mask] token으로 바꿈)
report_accuracy: true
# reset_dataloader: true
# reset_lr_scheduler: true
# reset_meters: true
# reset_optimizer: true
# restore_file: checkpoint.pt
save_dir: checkpoints/utut
task: utut_pretraining
tensorboard_logdir: exp/utut
# tokens_per_sample: 512
total_num_update: 500000
train_subset: train
update_freq:
-  8 #72
user_dir: ./
valid_subset: valid
keep_interval_updates: 3

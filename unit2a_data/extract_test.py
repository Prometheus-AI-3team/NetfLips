import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1" # GPU ì„¤ì •
import sys
import torch
import librosa
import numpy as np
from omegaconf import OmegaConf

# ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)
standard_paths = [p for p in sys.path if "site-packages" in p or "lib/python" in p]
for p in standard_paths:
    if p in sys.path:
        sys.path.remove(p)
        sys.path.insert(0, p)

av2av_root = "/home/2022113135/av2av"
fairseq_root = "/home/2022113135/av2av/fairseq"
task_dir = "/home/2022113135/av2av/av2unit"
hubert_pretrain_path = "/home/2022113135/av2av/av2unit/avhubert"

# ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
for p in [av2av_root, fairseq_root, task_dir, hubert_pretrain_path]:
    if p not in sys.path:
        sys.path.insert(0, p)
        
from fairseq import tasks

# íƒœìŠ¤í¬ ë³„ì¹­ ë“±ë¡
try:
    from hubert_pretraining import AVHubertPretrainingTask, AVHubertPretrainingConfig
    @tasks.register_task("av_hubert_unit_pretraining", dataclass=AVHubertPretrainingConfig)
    class AVHubertUnitPretrainingTaskAlias(AVHubertPretrainingTask):
        pass

    print("âœ… Successfully registered 'av_hubert_unit_pretraining' using class inheritance!")
except Exception as e:
    print(f"âš ï¸ Registration Bypass: {e}")

from fairseq import checkpoint_utils

# ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)
CHECKPOINT_PATH = "/home/2022113135/av2av/checkpoints/mavhubert_large_noise.pt"
LIST_FILE = "/home/2022113135/av2av/selected_files.txt"
OUT_DIR = "/home/2022113135/datasets/zeroth_test_units"
os.makedirs(OUT_DIR, exist_ok=True)

def load_mavhubert():
    print(f"Loading checkpoint from: {CHECKPOINT_PATH}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    state = checkpoint_utils.load_checkpoint_to_cpu(CHECKPOINT_PATH)
    cfg = state["cfg"]
    
    # OmegaConf ì—„ê²© ëª¨ë“œ í•´ì œ
    from omegaconf import OmegaConf
    OmegaConf.set_struct(cfg.model, False)
    
    # íƒœìŠ¤í¬ ìƒì„±
    task_dict = OmegaConf.to_container(cfg.task, resolve=True)

    task_dict["labels"] = [] 
    task_dict["label_dir"] = "/tmp" # ì•„ë¬´ ì˜ë¯¸ ì—†ëŠ” ê²½ë¡œë¡œ ì„¤ì •
    
    for k in ["pretrained_checkpoint", "noise_wav", "noise_prob", "noise_snr", "noise_num"]:
        task_dict.pop(k, None)
    task_obj = tasks.setup_task(OmegaConf.create(task_dict))
    
    # í´ë˜ìŠ¤ ë ˆë²¨ì—ì„œ dictionaries í”„ë¡œí¼í‹°ë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ê³ ì •
    type(task_obj).dictionaries = property(lambda self: [])
    
    # state ë‚´ë¶€ì— ì§ì ‘ ì£¼ì…
    if hasattr(task_obj, 'state'):
        task_obj.state.dictionaries = []
    
    # ëª¨ë¸ ì„¤ì • ìˆ˜ì •
    model_dict = OmegaConf.to_container(cfg.model, resolve=True)
    model_dict.update({"_name": "av_hubert", "final_dim": 256, "audio_embed_dim": 104})
    
    # Fairseqì˜ ìµœì‹  Transformer/Wav2Vec2 ì½”ë“œê°€ ìš”êµ¬í•˜ëŠ” ëª¨ë“  ê¸°ë³¸ ì„¤ì •ê°’
    missing_defaults = {
        # ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°
        "audio_embed_dim": 104, 
        "encoder_embed_dim": 1024,    
        "conv_pos": 128,
        "conv_pos_groups": 16,
        
        # ìœ ì‚¬ë„ ê´€ë ¨
        "sim_type": "cosine",
        "logit_temp": 0.1,
        "target_glu": False,
        "final_dim": 256,
        "untie_final_proj": True,
        
        # Masking ê´€ë ¨ 
        "mask_selection": "static",   
        "mask_other": 0.0,
        "mask_length": 10,
        "mask_prob": 0.8,
        "no_mask_overlap": False,
        "mask_min_space": 1,
        "mask_channel_selection": "static",
        "mask_channel_other": 0.0,
        "mask_channel_length": 10,
        "mask_channel_prob": 0.0,
        "no_mask_channel_overlap": False,
        "mask_channel_min_space": 1,
        
        # Transformer & Activation
        "activation_fn": "gelu",
        "layer_type": "transformer", 
        "layerdrop": 0.0,
        "checkpoint_activations": False,
        "offload_activations": False,
        
        # Convolution & ResNet
        "required_seq_len_multiple": 1,
        "conv_pos": 128,
        "conv_pos_groups": 16,   
        "resnet_relu_type": "prelu",
        "resnet_weights": None,
        
        # ê¸°íƒ€ í•„ìˆ˜ íŒŒë¼ë¯¸í„°
        "sub_encoder_layers": model_dict.get("encoder_layers", 24),
        "layer_norm_first": True,
        "feature_grad_mult": 1.0,
        "encoder_layerdrop": 0.0,
        "dropout_input": 0.0,
        "dropout_features": 0.0,
        "attention_dropout": 0.0,
        "activation_dropout": 0.0,
        "dropout": 0.0,
        "no_seed_provided": False, 
        "cond_on_norm": False,
        "reproducible": False,
        "encoder_layers_B": 0,
    }
    
    for k, v in missing_defaults.items():
        if k not in model_dict:
            model_dict[k] = v
    
    from hubert import AVHubertModel
    model = AVHubertModel.build_model(OmegaConf.create(model_dict), task_obj)
    
    # íŠ¹ì§• ì¶”ì¶œ í›„ [Batch, Time, Dim] -> [Batch, Time, 1024]ë¡œ ë°”ê¿”ì£¼ëŠ” ì—­í• 
    if hasattr(model.feature_extractor_audio, 'proj'):
        print("ğŸ”§ Patching audio feature extractor projection layer...")
    
    # ì²´í¬í¬ì¸íŠ¸ ë°˜ì˜ : 256ì°¨ì› ì„ë² ë”© ë ˆì´ì–´ ê°•ì œ ìƒì„±
    model.final_proj = torch.nn.Linear(1024, 256)
    
    print("--- Loading Weights & Injecting Codebook ---")
    ckpt_state = state["model"]
    model_state = model.state_dict()
    new_state_dict = {}
    
    model.unit_codebook = None
    model.label_predictor_weight = None

    for k, v in ckpt_state.items():
        # ì¼ë°˜ ë ˆì´ì–´ ë§¤í•‘
        if k in model_state and v.shape == model_state[k].shape:
            new_state_dict[k] = v
        
        # 256ì°¨ì› ì¶œë ¥ ë ˆì´ì–´ ì°¾ê¸°
        if "final_proj" in k or "label_predictor" in k:
            if v.shape == torch.Size([256, 1024]):
                new_state_dict["final_proj.weight"] = v
                print(f"ğŸ¯ Mapped {k} -> final_proj.weight")
            elif v.shape == torch.Size([256]):
                new_state_dict["final_proj.bias"] = v
                print(f"ğŸ¯ Mapped {k} -> final_proj.bias")

        # ìœ ë‹› ì½”ë“œë¶ ì°¾ê¸° ([2008, 256] ëª¨ì–‘)
        if v.shape == torch.Size([2008, 256]):
            model.unit_codebook = v.cuda() # GPUë¡œ ë¯¸ë¦¬ ì „ì†¡
            print(f"ğŸ”¥ Found Codebook: {k} (Size: 2008 units)")
        
        # final_projê°€ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„í•´ ê°€ì¤‘ì¹˜ ë³´ê´€
        if "label_predictor.weight" in k or "final_proj.weight" in k:
            if v.shape[0] == 256:
                model.label_predictor_weight = v.cuda()

    model.load_state_dict(new_state_dict, strict=False)
    
    if model.unit_codebook is None:
        raise ValueError("âŒ Critical: Could not find [2008, 256] codebook in checkpoint!")

    return model.cuda().eval()

print("\n--- Step 1: Loading Model ---")
model = load_mavhubert()

print("\n--- Step 2: Testing Top 10 Files ---")
with open(LIST_FILE, "r") as f:
    test_files = [line.strip() for line in f.readlines()[:10]]

print("\n--- Step 3: Extracting Units (Audio-Only Mode) ---")
for i, f in enumerate(test_files):
    try:
        audio, _ = librosa.load(f, sr=16000)
        
        # 104ì°¨ì› ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ
        mel = librosa.feature.melspectrogram(
            y=audio, sr=16000, n_mels=104, 
            n_fft=1024, hop_length=640, win_length=1024
        )
        
        # ë¡œê·¸ ë³€í™˜ í›„ ì •ê·œí™” (Mean 0, Var 1)
        log_mel = librosa.power_to_db(mel, ref=np.max)
        log_mel = (log_mel - np.mean(log_mel)) / (np.std(log_mel) + 1e-6)
        
        mel_tensor = torch.from_numpy(log_mel).float().cuda().unsqueeze(0) # [1, 104, T]

        with torch.no_grad():
            # ëª¨ë¸ì˜ projection ë ˆì´ì–´ í†µê³¼ [B, 104, T] -> [B, T, 1024]
            x = mel_tensor.transpose(1, 2) 
            res_x = model.feature_extractor_audio.proj(x) 
            
            # Encoder í†µê³¼ ë° ìœ ë‹› ì¶”ì¶œ
            res_x, _ = model.encoder(res_x, padding_mask=None)
            
            if hasattr(model, 'final_proj'):
                emb = model.final_proj(res_x)
            else:
                emb = torch.nn.functional.linear(res_x, model.label_predictor_weight)
            
            # ì„ë² ë”© ê°’ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸
            print(f"DEBUG: Emb Mean: {emb.mean().item():.4f}, Std: {emb.std().item():.4f}, Max: {emb.max().item():.4f}")
            
            dists = torch.cdist(emb.squeeze(0), model.unit_codebook)
            units = torch.argmin(dists, dim=-1).flatten().cpu()
        
        # ìœ ë‹› ê²€ì¦ ë¡œê·¸
        unique_units = torch.unique(units)
        print(f"[{i+1}/10] âœ… Done: {os.path.basename(f)} | Unique: {len(unique_units)} | Sample: {units[10:20].tolist()}")
            
        # ì €ì¥
        save_data = {
            'code': units.to(torch.long),
            'spkr': torch.zeros(256).float(),
            'f0': torch.zeros(len(units)).float(),
            'dur_prediction': False
        }
        
        save_path = os.path.join(OUT_DIR, os.path.basename(f).replace(".wav", ".pt"))
        torch.save(save_data, save_path)
        print(f"[{i+1}/10] âœ… Done: {os.path.basename(f)} ({len(units)} units)")
        
    except Exception as e:
        import traceback
        print(f"[{i+1}/10] âŒ Error: {e}")
        traceback.print_exc()
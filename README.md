# ğŸ¬ NetfLips

**Unit-based Audiovisual Translation for Korean**  
*Text-free Direct Speech Translation with Synchronized Lip Movement*

<div align="center">
  
  [![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
  [![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
  
</div>

---

## ğŸ“‹ Overview

NetfLipsëŠ” ì˜ì–´ ì˜ìƒì„ ì…ë ¥ë°›ì•„ **ìŒì„±ê³¼ ì… ëª¨ì–‘ì´ ë™ê¸°í™”ëœ í•œêµ­ì–´ ë²ˆì—­ ì˜ìƒ**ì„ ìƒì„±í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

### âœ¨ Key Features

- **ğŸ¯ Unit-based Translation**: í…ìŠ¤íŠ¸ ì¤‘ê°„ í‘œí˜„ ì—†ì´ ìŒì„±ê³¼ ì‹œê° ì •ë³´ë¥¼ ê³µí†µ ìœ ë‹›(Unit) í‘œí˜„ìœ¼ë¡œ ì§ì ‘ ëª¨ë¸ë§
- **ğŸ”Š Speech & Visual Sync**: ìŒì„±ê³¼ ë¹„ë””ì˜¤ë¥¼ ê³µí†µ íŠ¹ì§• ê³µê°„ì˜ Unit ë‹¨ìœ„ë¡œ ì •ë ¬í•˜ì—¬ ê°•ê±´í•œ ë²ˆì—­ êµ¬í˜„
- **ğŸ‡°ğŸ‡· Korean Fine-tuning**: ê¸°ì¡´ì— ì§€ì›ë˜ì§€ ì•Šë˜ í•œêµ­ì–´ capabilityë¥¼ ìœ„í•œ Fine-tuning
- **ğŸ’¬ Natural Synthesis**: ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í•©ì„± ë° ë¦½ì‹±í¬ ìƒì„±

### ğŸ¯ Keywords

`#Unit-based Audiovisual Translation` `#Text-free Direct Speech Translation` `#Lip Sync` `#Speech Translation`

---

## ğŸ¥ Demo
ğŸŒ **[Demo Link](https://www.miricanvas.com/v2/design2/v/db519087-2eae-4b07-9b24-9d925d81469f)**
---

## ğŸ—ï¸ Architecture

NetfLipsëŠ” 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

### 1ï¸âƒ£ Unit Extraction
- FLAC ë³µì› (wav)
- íŠ¹ì§• ì¶”ì¶œ (Mel Spectrogram)
- K-means ë¶„ë¥˜
- ì •ìˆ˜ sequenceë¡œ ë³€í™˜

### 2ï¸âƒ£ Unit Translation
- **Base Model**: AV2AV (Choi, J., et al., 2024)
- **Translation**: ì˜ì–´ unit â†’ í•œêµ­ì–´ unit
- **Framework**: Fairseq toolkit ê¸°ë°˜ unit sequence í•™ìŠµ
- **Backbone**: ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ mBART í™œìš©

### 3ï¸âƒ£ AV Generation
- Unit â†’ Audio ë³€í™˜
- í•œêµ­ì–´ unit & í™”ì ì„ë² ë”© í™œìš©
- Speech Resynthesis

---

## ğŸ“Š Dataset

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤:

| Dataset | Description | Size |
|---------|-------------|------|
| **Zeroth Korean ASR** | í•œêµ­ì–´ ìŒì„± ì¸ì‹ ë°ì´í„° | 12,245 ë¬¸ì¥ |
| **AIHub Ko-X í†µë²ˆì—­ ìŒì„±** | í•œêµ­ì–´-ì˜ì–´(ë¯¸êµ­) ë³‘ë ¬ ìŒì„± ë°ì´í„° | 169,488 ë¬¸ì¥ |


## ğŸš€ Getting Started

### Prerequisites

```bash
# 1. ë ˆí¬ì§€í† ë¦¬ í´ë¡ 
git clone https://github.com/Prometheus-AI-3team/NetfLips.git

cd NetfLips

# 2. ì„œë¸Œëª¨ë“ˆ(fairseq) update
git submodule init
git submodule update

# 2. Conda ê¸°ë³¸ í™˜ê²½ ìƒì„±
conda env create -f environment.yml
conda activate unit2a

# 3. Pip ë‹¤ìš´ê·¸ë ˆì´ë“œ (ë©”íƒ€ë°ì´í„° ì—ëŸ¬ ë°©ì§€)
pip install "pip<24.1"

# 4. PyTorch ì„¤ì¹˜ (CUDA 11.7 ê¸°ì¤€)
pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117
```

### Installation

```bash
# 5. ë‚˜ë¨¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -r requirements.txt

# 6. Fairseq ì„¤ì¹˜
cd av2av-main/fairseq
pip install -e .
```

---

## ğŸ’» Usage
### Checkpoints
| Model | Name | link |
| --- | --- | --- |
| AV2Unit | `mav_hubert_large_noise.py` | [download]() |
| Unit2Unit | `utut_sts_ft.pt` | [download]() |
| Unit2AV | `unit_av_renderer_withKO.pt` | [download](https://drive.google.com/file/d/1vNaJGWqqC8VAzEXTEYb33fq5PsfE74F6/view?usp=drive_link) |

### Quick Start

```python
# ì‚¬ìš© ì˜ˆì œ ì½”ë“œ (ì¶”í›„ ì—…ë°ì´íŠ¸)
```

### Advanced Usage

```bash
# ì»¤ë§¨ë“œë¼ì¸ ì‚¬ìš©ë²• (ì¶”í›„ ì—…ë°ì´íŠ¸)
```

---

## ğŸ“ Project Structure

```
NetfLips/
â”œâ”€â”€ av2unit/                  # Audio-Visual to Unit Extraction
â”‚   â”œâ”€â”€ avhubert/             # Feature extraction using AV-HuBERT
â”‚   â””â”€â”€ inference.py          # Unit extraction inference script
â”œâ”€â”€ unit2unit/                # Unit to Unit Translation
â”‚   â”œâ”€â”€ utut_pretrain/        # Pre-training modules
â”‚   â”œâ”€â”€ utut_finetune/        # Fine-tuning modules
â”‚   â””â”€â”€ inference.py          # Translation inference script
â”œâ”€â”€ unit2av/                  # Unit to Audio-Visual Generation
â”‚   â”œâ”€â”€ model.py              # Unit2AV model definition
â”‚   â”œâ”€â”€ train_unit2a.py       # Training script for Unit2Audio
â”‚   â””â”€â”€ inference_unit2av.py  # Inference scripts
â”œâ”€â”€ fairseq/                  # Fairseq Toolkit (Submodule)
â”œâ”€â”€ scripts/                  # Utility Scripts for Data Preparation
â”œâ”€â”€ inference_av2av.py        # Main End-to-End Inference Script
â”œâ”€â”€ environment.yml           # Conda Environment Configuration
â””â”€â”€ requirements.txt          # Python Dependencies
```

---

## ğŸ”¬ Methodology

### Data Preprocessing
- FLAC íŒŒì¼ ë³µì› ë° wav ë³€í™˜
- Mel Spectrogram ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
- K-means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ Unit ë¶„ë¥˜

### Model Training
- mBART ê¸°ë°˜ sequence-to-sequence í•™ìŠµ
- Fairseq toolkit í™œìš©
- Unit-to-Unit translation ìµœì í™”

### Audio-Visual Generation
- í•œêµ­ì–´ unitì—ì„œ ìŒì„± ì¬í•©ì„±
- í™”ì ì„ë² ë”©ì„ í™œìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ìƒì„±
- ë¦½ì‹±í¬ê°€ ë™ê¸°í™”ëœ ë¹„ë””ì˜¤ ìƒì„±

---

## ğŸ› ï¸ Technical Details

### Base Model
- **AV2AV**: Audio-Visual to Audio-Visual translation model
- **Reference**: Choi, J., et al., 2024

### Fine-tuning Strategy
- í•œêµ­ì–´ ë¯¸ì§€ì› ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ Fine-tuning
- ë³‘ë ¬ í•œ-ì˜ ìŒì„± ë°ì´í„° í™œìš©
- Unit-level translation í•™ìŠµ

---

## ğŸ‘¥ Team Members From Prometheus(AI club)

| Name | batch |
|------|------|
| **ì¥ì§€ìˆ˜** | 6th |
| **ìœ ì§€í˜œ** | 6th |
| **ì‹ ê·œì² ** | 8th |
| **ì´ê°€ì—°** | 8th |

---

## ğŸ“ Citation

```bibtex
@misc{netflips2024,
  title={NetfLips: Unit-based Audiovisual Translation for Korean},
  author={ì¥ì§€ìˆ˜, ìœ ì§€í˜œ, ì‹ ê·œì² , ì´ê°€ì—°},
  year={2024}
}
```

### References
- Choi, J., et al. (2024). AV2AV: Audio-Visual to Audio-Visual Translation

---

## License

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## Acknowledgments

This repository is built upon [AV2AV](https://github.com/choijeongsoo/av2av?tab=readme-ov-file) and [Fairseq](https://github.com/pytorch/fairseq). We appreciate the open-source of the projects.
